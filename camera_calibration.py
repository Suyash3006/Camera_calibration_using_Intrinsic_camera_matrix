# -*- coding: utf-8 -*-
"""Camera_calibration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iPnvu3v4fiDShuAK4YZkEPAnJ149QAxO

# **Camera Calibration**
"""

import cv2
# import cv
import numpy as np
import matplotlib.pyplot as plt

"""### Load Image files and Intrinsic Camera Matrix"""

# Load images im1 and im2
img1 = cv2.imread("/content/drive/MyDrive/auto_programing_assignment/im1.jpg" , 0)
img2 = cv2.imread("/content/drive/MyDrive/auto_programing_assignment/im2.jpg" , 0)

# Load intrinsic camera matrix 
Intrinsic_camera_matrix = np.loadtxt("/content/drive/MyDrive/auto_programing_assignment/Intrinsic_Matrix_K.txt")
print(Intrinsic_camera_matrix)

from google.colab import drive
drive.mount('/content/drive')

"""### Ground Truth Correspondences Extraction via Feature Detectors"""

def length_check(Sift_correspondences):
    sift_length = len(Sift_correspondences)
    if sift_length < 100 :
        print("There are not enough of the correspondences present in the immages.")
        exit()
    return True

def correspondence_fit(match , Sift_correspondences):
    for matpoints in range(len(match)):
        if ((match[matpoints][0].distance) < 0.75*(match[matpoints][1].distance)):
            Sift_correspondences.append(match[matpoints][0])
    return Sift_correspondences

# Use SIFT feature detector to find correspondences of the Key points to the image
sift_model = cv2.SIFT_create() # here SIFT stands for the  Scale Invariant Feature Transform
keypoints_imag1_Sift, Descriptor_sift_im1 = sift_model.detectAndCompute(img1, None)
keypoints_imag2_Sift, Descriptor_sift_im2 = sift_model.detectAndCompute(img2, None)
#use ORB feature detector to find correspondences of the Key points to the image
ORB_model = cv2.ORB_create() # ORB is Oriented Fast and Rotated Brief
keypoints_imag1_ORB, Descriptor_ORB_im1 = ORB_model.detectAndCompute(img1, None)
keypoints_imag2_ORB, Descriptor_ORB_im2 = ORB_model.detectAndCompute(img2, None)
# BF matcher is the Brute Force Matcher it is basicallu used for
# matching the descriptors of the one feature with the other on the basis of the
# distance calculation 
Brute_force_matche_check_model = cv2.BFMatcher()    
matches_SIFT_model = Brute_force_matche_check_model.knnMatch(Descriptor_sift_im1, Descriptor_sift_im2, k = 2)
matches_ORB_model  = Brute_force_matche_check_model.knnMatch(Descriptor_ORB_im1 , Descriptor_ORB_im2 , k = 2)
Sift_correspondences = []
ORB_correspondences = []

Sift_correspondences = correspondence_fit(matches_SIFT_model , Sift_correspondences);
ORB_correspondences = correspondence_fit(matches_ORB_model , ORB_correspondences);

# There were more than 100 correspondences needed so we have to check whether there are 100 such correspondences
fit = length_check(Sift_correspondences)
print(Descriptor_ORB_im1)
print(Descriptor_ORB_im2)
print("\n")
print(Descriptor_sift_im1)
print(Descriptor_sift_im2)
print("\n")
print(Sift_correspondences)
print("\n")
print(ORB_correspondences)

"""#### SIFT_feature_detector_image1"""

img_keypoints_sift_imag1 = cv2.drawKeypoints(img1, keypoints_imag1_Sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_ = cv2.resize(img_keypoints_sift_imag1 , (2000 , 1500))
plt.imshow(img_)
plt.show()

"""#### SIFT_feature_detector_image2"""

img_keypoints_sift_imag1 = cv2.drawKeypoints(img2, keypoints_imag2_Sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_ = cv2.resize(img_keypoints_sift_imag1 , (2000 , 1500))
plt.imshow(img_)
plt.show()

"""#### ORB_feature_detector_image1"""

img_keypoints_ORB_imag1 = cv2.drawKeypoints(img1, keypoints_imag1_ORB, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_ = cv2.resize(img_keypoints_ORB_imag1 , (2000 , 1500))
plt.imshow(img_)
plt.show()

"""#### ORB_feature_detector_image2"""

img_keypoints_ORB_imag2 = cv2.drawKeypoints(img2, keypoints_imag2_ORB, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_ = cv2.resize(img_keypoints_ORB_imag2 , (2000 , 1500))
plt.imshow(img_)
plt.show()

"""#### Correspondence Matching in SIFT"""

Indexing_parameters_SIFT = dict(algorithm=0, trees=50)
search_params_SIFT = dict(checks=150)   # or pass empty dictionary
  
flann_SIFT = cv2.FlannBasedMatcher(Indexing_parameters_SIFT, search_params_SIFT)
  
Matches_SIFT = flann_SIFT.knnMatch(Descriptor_sift_im1, Descriptor_sift_im2, k=2)
  
# Need to draw only good matches, so create a mask
good_matches_SIFT = [[0, 0] for i in range(len(Matches_SIFT))]
  
# Good matches
for i, (m, n) in enumerate(Matches_SIFT):
    if m.distance < 0.75*n.distance:
        good_matches_SIFT[i] = [1, 0]
  
  
# Draw the matches using drawMatchesKnn()
Matched_points_SIFT = cv2.drawMatchesKnn(img1,
                             keypoints_imag1_Sift,
                             img2,
                             keypoints_imag2_Sift,
                             Matches_SIFT,
                             outImg=None,
                             matchColor = None,
                             singlePointColor=(0, 255, 255),
                             matchesMask=good_matches_SIFT,
                             flags=0
                             )
  
# Displaying the image
cv2.resize(Matched_points_SIFT , (2000 , 2000))
plt.figure().set_figheight(5000)
plt.figure().set_figheight(500)

plt.imshow(Matched_points_SIFT)
plt.show()

Indexing_parameters_ORB = dict(algorithm=0, trees=70)
search_params_ORB = dict(checks=150)   # or pass empty dictionary
  
flann_ORB = cv2.FlannBasedMatcher(Indexing_parameters_ORB, search_params_ORB)
  
# Matches_ORB = flann_ORB.knnMatch(Descriptor_ORB_im1, Descriptor_ORB_im2, k=2)
  
# Need to draw only good matches, so create a mask
good_matches_ORB = [[0, 0] for i in range(len(matches_ORB_model))]
  
# Good matches
for i, (m, n) in enumerate(matches_ORB_model):
    if m.distance < 0.75*n.distance:
        good_matches_ORB[i] = [1, 0]
  
  
# Draw the matches using drawMatchesKnn()
Matched_points_ORB = cv2.drawMatchesKnn(img1,
                             keypoints_imag1_ORB,
                             img2,
                             keypoints_imag2_ORB,
                             matches_ORB_model,
                             outImg=None,
                             matchColor = None,
                             singlePointColor=(0, 255, 255),
                             matchesMask=good_matches_ORB,
                             flags=0
                             )
  
# Displaying the image
cv2.resize(Matched_points_ORB , (2000 , 2000))
plt.figure().set_figheight(5000)
plt.figure().set_figheight(500)

plt.imshow(Matched_points_ORB)
plt.show()

"""### Essentinal Matrix Computation Method"""

def computeEssentialMatrix(points1, points2, camera_matrix):
    # Normalize image coordinates
    Essetnial_matrix_check_fit, Intelier = cv2.findEssentialMat(points1, points2, camera_matrix)
    if(Essetnial_matrix_check_fit.all() != None):
        return Essetnial_matrix_check_fit;
    key_points1_norm = np.linalg.inv(camera_matrix) @ np.hstack((points1, np.ones((points1.shape[0],1)))).T
    key_points1_norm /= np.linalg.norm(key_points1_norm, axis=0)
    key_points2_norm = np.linalg.inv(camera_matrix) @ np.hstack((points2, np.ones((points2.shape[0],1)))).T
    key_points2_norm /= np.linalg.norm(key_points2_norm, axis=0)
    print(key_points1_norm.shape , key_points2_norm.shape)
    print(key_points1_norm)
    print(key_points2_norm)
    # Compute the fundamental matrix
    A = np.vstack((key_points1_norm.T * key_points2_norm, key_points1_norm.T * key_points2_norm, key_points1_norm.T)).T
    _, _, v = np.linalg.svd(A)
    F = v[-1,:].reshape(3,3)

    # Enforce rank-2 constraint on fundamental matrix
    u, s, v = np.linalg.svd(F)
    s[2] = 0
    F = u @ np.diag(s) @ v

    # Compute essential matrix
    E_ = camera_matrix.T @ F @ camera_matrix
    return E

"""#### Pixel Coordinates extraction"""

def pixel_coordinates_extractor(keypoints_imag1 , keypoints_imag2 , Corr_Id):
    Pixel_point1 = np.float32([keypoints_imag1[elem.queryIdx].pt for elem in Corr_Id]).reshape(-1 , 1 , 2)
    Pixel_point2 = np.float32([keypoints_imag2[elem.trainIdx].pt for elem in Corr_Id]).reshape(-1 , 1 , 2)
    return Pixel_point1 , Pixel_point2

def flatten_matrix(pts1):   
    mat = np.zeros((1272 , 2))
    for i in range(len(pts1)):
        mat[i][0] = pts1[i][0][0]
        mat[i][1] = pts1[i][0][1]
    return mat
# flatten_pts1 = flatten_matrix(pts1)
# flatten_pts2 = flatten_matrix(pts2)

# Extracting  pixel coordinates of correspondences
Pixel_point_sift_imag1 , Pixel_point_sift_imag2 = pixel_coordinates_extractor(keypoints_imag1_Sift , keypoints_imag2_Sift , Sift_correspondences)
Pixel_point_ORB_imag1 , Pixel_point_ORB_imag2 = pixel_coordinates_extractor(keypoints_imag1_ORB , keypoints_imag2_ORB , ORB_correspondences)

## essential Matrix Computation
Essential_matrix_Sift_feature = computeEssentialMatrix(Pixel_point_sift_imag1 ,Pixel_point_sift_imag2 , Intrinsic_camera_matrix)
Essential_matrix_ORB_feature = computeEssentialMatrix(Pixel_point_ORB_imag1 , Pixel_point_ORB_imag2 , Intrinsic_camera_matrix)
print(Essential_matrix_Sift_feature)
print("\n")
print(Essential_matrix_ORB_feature)
print(Pixel_point_sift_imag1 , Pixel_point_sift_imag2)
print(Pixel_point_ORB_imag1 , Pixel_point_ORB_imag2)
print("Shapes of the Pixel points of image1 and Image 2 respectively(SIFT_feature)  : ",Pixel_point_sift_imag1.shape , Pixel_point_sift_imag2.shape)
print("Shapes of the Pixel points of image1 and Image 2 respectively(ORB_feature)  : ",Pixel_point_ORB_imag1.shape , Pixel_point_ORB_imag2.shape)

"""### Decomposition of Essential Matrix for the SIFT and ORB Features"""

# Computation of the QR Decomsposition of the Essential Matrix
Img_eePoints_SIFT ,  Camera_Rotation_matrix_SIFT, Camera_translation_matrix_SIFT, Inteliers_SIFT = cv2.recoverPose(Essential_matrix_Sift_feature, Pixel_point_sift_imag1, Pixel_point_sift_imag2, Intrinsic_camera_matrix)
Img_eePoints_ORB , Camera_Rotation_matrix_ORB , Camera_translation_matrix_ORB , Inteliers_ORB = cv2.recoverPose(Essential_matrix_ORB_feature , Pixel_point_ORB_imag1 , Pixel_point_ORB_imag2 , Intrinsic_camera_matrix)
if (Img_eePoints_SIFT == None):
    print("Decomposition exception acquired  in SIFT")
    exit()
if (Img_eePoints_ORB == None):
    print("Decomposition exception acquired in ORB")
# output Rotation and translation matrices after the decomposition of the matrices.    
print(Camera_Rotation_matrix_SIFT ,"\n","\n", Camera_translation_matrix_SIFT)
print("\n")
print(Camera_Rotation_matrix_ORB ,"\n","\n", Camera_translation_matrix_ORB)

"""### Triangulation of 3D points"""

def triangulatePoints(proj_mat1, proj_mat2, points1, points2):
    # Convert points to homogeneous coordinates
    P = cv2.triangulatePoints(proj_mat1, proj_mat2, points1,points2)
    if(P.all() != None):
        return P;
    points1_hom = np.hstack([points1, np.ones((points1.shape[0], 1))])
    points2_hom = np.hstack([points2, np.ones((points2.shape[0], 1))])

    # Compute the homogeneous transformation matrix for each camera
    R1 = proj_mat1[:, :3]
    t1 = proj_mat1[:, 3]
    T1 = np.hstack([R1, t1.reshape(3, 1)])
    P1 = np.eye(4)
    P1[:3, :] = T1
    R2 = proj_mat2[:, :3]
    t2 = proj_mat2[:, 3]
    T2 = np.hstack([R2, t2.reshape(3, 1)])
    P2 = np.eye(4)
    P2[:3, :] = T2

    # Triangulate points
    num_points = points1_hom.shape[0]
    triangulated_pts = np.zeros((num_points, 3))
    for i in range(num_points):
        A = np.vstack([points1_hom[i], points2_hom[i]])
        W, _, V = np.linalg.svd(A)
        X_hom = V[-1, :]
        X_hom /= X_hom[3]
        X = X_hom[:3]
        triangulated_pts[i] = X

    return triangulated_pts

"""#### 3D points Mapping via Pixel coordinates and camera orientation Triangulation Therorem"""

Rotation_Identity_matrix = np.eye(4)
Rotation_Identity_matrix[3][3] = 0
Rotation_Identity_matrix = Rotation_Identity_matrix[:3 , :]
# Id_matrix = np.stack((Id_matrix , t_vec))
## for the first camera rotation translation matrices are I and 0 matrix respectively.
Rotated_translatory_matrix_SIFT_feature = np.hstack((Camera_Rotation_matrix_SIFT,Camera_translation_matrix_SIFT))
Rotated_translatory_matrix_ORB_feature = np.hstack((Camera_Rotation_matrix_ORB,Camera_translation_matrix_ORB))

P_3dPoints_SIFT_feature_extractor = triangulatePoints(Rotation_Identity_matrix , Rotated_translatory_matrix_SIFT_feature , Pixel_point_sift_imag1 , Pixel_point_sift_imag2)
P_3dPoints_ORB_feature_extractor = triangulatePoints(Rotation_Identity_matrix , Rotated_translatory_matrix_ORB_feature , Pixel_point_ORB_imag1 , Pixel_point_ORB_imag2)

# In order to normalize the points in the 3d plane we need to make the all the coordinates divided by the third coordinate p = p/p[3]
P_3dPoints_SIFT_feature_extractor = P_3dPoints_SIFT_feature_extractor / P_3dPoints_SIFT_feature_extractor[3]
P_3dPoints_ORB_feature_extractor = P_3dPoints_ORB_feature_extractor / P_3dPoints_ORB_feature_extractor[3]

print(Rotated_translatory_matrix_SIFT_feature ,"\n","\n", Rotated_translatory_matrix_ORB_feature)
print("\n")
print(P_3dPoints_SIFT_feature_extractor ,"\n","\n", P_3dPoints_ORB_feature_extractor)
print("\n")
print(P_3dPoints_SIFT_feature_extractor.shape , P_3dPoints_ORB_feature_extractor.shape)

"""### Plotting the 3D-Points(Pi) and Camera center (T)

#### ORB features 3D points Plot
"""

# Plot 3D points and camera center
_3d_graph_plot = plt.figure()
_3d_marker= _3d_graph_plot.add_subplot(projection='3d')
# camera_center = graph.add_subplot(projection = '3d')
_3d_marker.scatter(P_3dPoints_ORB_feature_extractor[0], P_3dPoints_ORB_feature_extractor[1], P_3dPoints_ORB_feature_extractor[2], c='Green', marker='*')
_3d_marker.set_xlabel('X')
_3d_marker.set_ylabel('Y')
_3d_marker.set_zlabel('Z')
plt.title('3D_ points for ORB')

plt.show()

# Plot 3D points and camera center
camera_center_plot = plt.figure()
camera_center = camera_center_plot.add_subplot(projection = '3d')
camera_center.scatter(P_3dPoints_ORB_feature_extractor[0], P_3dPoints_ORB_feature_extractor[1], P_3dPoints_ORB_feature_extractor[2], c='Green', marker='*')
camera_center.scatter(Camera_translation_matrix_ORB[0], Camera_translation_matrix_ORB[1], Camera_translation_matrix_ORB[2], c='red', marker='x')
#the Red cross in the Graph is the camera center 

camera_center.set_xlabel('X')
camera_center.set_ylabel('Y')
camera_center.set_zlabel('Z')
plt.title('Camera_center with the 3d points ORB')

"""#### SIFT features 3D points Plot"""

# Plot 3D points and camera center
_3d_graph_plot = plt.figure()
_3d_marker= _3d_graph_plot.add_subplot(projection='3d')
# camera_center = graph.add_subplot(projection = '3d')
_3d_marker.scatter(P_3dPoints_SIFT_feature_extractor[0], P_3dPoints_SIFT_feature_extractor[1], P_3dPoints_SIFT_feature_extractor[2], c='Yellow', marker='*')
_3d_marker.set_xlabel('X')
_3d_marker.set_ylabel('Y')
_3d_marker.set_zlabel('Z')
plt.title('3D_ points for SIFT')

plt.show()

# Plot 3D points and camera center
camera_center_plot = plt.figure()
camera_center = camera_center_plot.add_subplot(projection = '3d')
camera_center.scatter(P_3dPoints_SIFT_feature_extractor[0], P_3dPoints_SIFT_feature_extractor[1], P_3dPoints_SIFT_feature_extractor[2], c='Yellow', marker='*')
camera_center.scatter(Camera_translation_matrix_SIFT[0], Camera_translation_matrix_SIFT[1], Camera_translation_matrix_SIFT[2], c='Black', marker='x')
#the Red cross in the Graph is the camera center 

camera_center.set_xlabel('X')
camera_center.set_ylabel('Y')
camera_center.set_zlabel('Z')
plt.title('Camera_center with the 3d points SIFT')